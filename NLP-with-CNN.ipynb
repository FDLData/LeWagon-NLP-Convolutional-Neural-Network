{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP with CNN\n",
    "\n",
    "### Exercise objectives:\n",
    "\n",
    "- Use CNN instead of RNN for NLP\n",
    "\n",
    "<hr>\n",
    "<hr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# The data\n",
    "\n",
    "\n",
    "‚ùì **Question** ‚ùì Let's first load the data. You don't have to understand what is going on in the function, it does not matter here.\n",
    "\n",
    "‚ö†Ô∏è **Warning** ‚ö†Ô∏è The `load_data` function has a `percentage_of_sentences` argument. Depending on your computer, there are chances that too many sentences will make your compute slow down, or even freeze - your RAM can overflow. For that reason, **you should start with 10% of the sentences** and see if your computer handles it. Otherwise, rerun with a lower number. \n",
    "\n",
    "‚ö†Ô∏è **DISCLAIMER** ‚ö†Ô∏è **No need to play _who has the biggest_ (RAM) !** The idea is to get to run your models quickly to prototype. Even in real life, it is recommended that you start with a subset of your data to loop and debug quickly. So increase the number only if you are into getting the best accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "### Run this cell to load the data ###\n",
    "######################################\n",
    "\n",
    "import tensorflow_datasets as tfds\n",
    "from tensorflow.keras.preprocessing.text import text_to_word_sequence\n",
    "\n",
    "def load_data(percentage_of_sentences=None):\n",
    "    train_data, test_data = tfds.load(name=\"imdb_reviews\", split=[\"train\", \"test\"], batch_size=-1, as_supervised=True)\n",
    "\n",
    "    train_sentences, y_train = tfds.as_numpy(train_data)\n",
    "    test_sentences, y_test = tfds.as_numpy(test_data)\n",
    "    \n",
    "    # Take only a given percentage of the entire data\n",
    "    if percentage_of_sentences is not None:\n",
    "        assert(percentage_of_sentences> 0 and percentage_of_sentences<=100)\n",
    "        \n",
    "        len_train = int(percentage_of_sentences/100*len(train_sentences))\n",
    "        train_sentences, y_train = train_sentences[:len_train], y_train[:len_train]\n",
    "  \n",
    "        len_test = int(percentage_of_sentences/100*len(test_sentences))\n",
    "        test_sentences, y_test = test_sentences[:len_test], y_test[:len_test]\n",
    "    \n",
    "    X_train = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in train_sentences]\n",
    "    X_test = [text_to_word_sequence(_.decode(\"utf-8\")) for _ in test_sentences]\n",
    "    \n",
    "    return X_train, y_train, X_test, y_test\n",
    "\n",
    "X_train, y_train, X_test, y_test = load_data(percentage_of_sentences=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that to do NLP, you need to go through one of the following options, as shown here: \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "But in both cases, you can replace the recurrent layer (top part) by a CNN layer. We will go into both, starting from the one on the left were the embedding is learned in the network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 1 : Concatenate a Keras Embedding with a Conv1Düî• \n",
    "\n",
    "Let's train a fancy network here. \n",
    "\n",
    "Each of your words is represented by a vector of size N (the size of your embedding). Therefore, as a sentence is a sequence of words, it is represented by a matrix (number of words, N). So, all your sentences are actually represented as matrices once embedded.\n",
    "\n",
    "If you think about it, an image is also a matrix. Said differently, you may represent your sentence of word as a matrix, where each column (or row, depending on how you want to look at it) is a word, and each row (or each column) corresponds to a coordinate in the embedding space. As shown here\n",
    "\n",
    "<img src=\"image_comparison.png\" width=\"500px\" />\n",
    "\n",
    "Well, in that case, as these are close to images, why not using convolution on them? Yes, convolutions!\n",
    "But, be careful. In the case of images, convolutions are 2 dimensional as the filters can move up and down, and left and right. In the case of our sentences, we want the kernel to move _only_ in the word by word direction (The alternative, moving coordinate by coordinate of the embedding space doesn't make much sense).\n",
    "\n",
    "So let's create a model that use convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## First, the data\n",
    "\n",
    "‚ùì **Question** ‚ùì You will need to prepare your data. First, tokenize them. Then, you need to pad them (use a value `maxlen` equal to 150). You also might need to compute the size of your vocabulary ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "\n",
    "\n",
    "tk = Tokenizer()\n",
    "tk.fit_on_texts(X_train)\n",
    "\n",
    "X_train_token = tk.texts_to_sequences(X_train)\n",
    "X_test_token = tk.texts_to_sequences(X_test)\n",
    "\n",
    "X_train_pad = pad_sequences(X_train_token, dtype='float32', padding='post', maxlen=150)\n",
    "X_test_pad = pad_sequences(X_test_token, dtype='float32', padding='post', maxlen=150)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "30419"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vocab_size = len(tk.word_index)\n",
    "vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using 1D Convolution.\n",
    "\n",
    "‚ùì **Question** ‚ùì Define a model that has :\n",
    "- an `Embedding` layer: `input_dim` is the `vocab_size + 1`, `output_dim` is the embedding space dimension, and `mask_zero` has to be set to `True`. Here, for computational reasons, set `input_length` to the maximum length of your observations (that you just defined in the previous question).\n",
    "- a `Conv1D` layer \n",
    "- a `Flatten` layer\n",
    "- a `Dense` layer\n",
    "- an output layer\n",
    "\n",
    "Compile the model accordingly\n",
    "\n",
    "‚ùó **Remark** ‚ùó The size of the `Conv1D` kernel corresponds exactly to the number of side-by-side words (tokens) each kernel is taking into account ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras import models\n",
    "\n",
    "model_cnn = models.Sequential()\n",
    "\n",
    "model_cnn.add(layers.Embedding(input_dim=vocab_size+1,\n",
    "                          output_dim=10,\n",
    "                          mask_zero=True,\n",
    "                          input_length=150))\n",
    "\n",
    "model_cnn.add(layers.Conv1D(16, 3))\n",
    "model_cnn.add(layers.Flatten())\n",
    "model_cnn.add(layers.Dense(5,))\n",
    "model_cnn.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Look at the number of parameters. You can compare it to the model that you had in previous exercise (esp. the first one)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " embedding (Embedding)       (None, 150, 10)           304200    \n",
      "                                                                 \n",
      " conv1d (Conv1D)             (None, 148, 16)           496       \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2368)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 5)                 11845     \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1)                 6         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 316,547\n",
      "Trainable params: 316,547\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model_cnn.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Fit your model with a stopping criterion, and evaluate it on the test data.\n",
    "\n",
    "You will probably notice that it is ... **much faster** than RNN."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 1s 3ms/step - loss: 0.6918 - accuracy: 0.5097 - val_loss: 0.6946 - val_accuracy: 0.4987\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.6389 - accuracy: 0.7549 - val_loss: 0.6533 - val_accuracy: 0.6507\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4563 - accuracy: 0.8954 - val_loss: 0.5391 - val_accuracy: 0.7453\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.2209 - accuracy: 0.9549 - val_loss: 0.5448 - val_accuracy: 0.7200\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0857 - accuracy: 0.9829 - val_loss: 0.5299 - val_accuracy: 0.7560\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0286 - accuracy: 0.9960 - val_loss: 0.4852 - val_accuracy: 0.7947\n",
      "Epoch 7/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0088 - accuracy: 0.9994 - val_loss: 0.5208 - val_accuracy: 0.7973\n",
      "Epoch 8/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.0028 - accuracy: 0.9994 - val_loss: 0.5718 - val_accuracy: 0.7947\n",
      "Epoch 9/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 5.9078e-04 - accuracy: 1.0000 - val_loss: 0.6421 - val_accuracy: 0.7987\n",
      "Epoch 10/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 3.5458e-04 - accuracy: 1.0000 - val_loss: 0.6775 - val_accuracy: 0.8000\n",
      "Epoch 11/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 4.0705e-05 - accuracy: 1.0000 - val_loss: 0.7268 - val_accuracy: 0.7933\n",
      "The accuracy evaluated on the test set is of 78.120%\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "\n",
    "es = EarlyStopping(patience=5, restore_best_weights=True)\n",
    "\n",
    "model_cnn.fit(X_train_pad, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=[es]\n",
    "         )\n",
    "\n",
    "res = model_cnn.evaluate(X_test_pad, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 2 : Learn a Word2Vec representation, and then feed it into a NN with a `Conv1D`üî• \n",
    "\n",
    "In the first part of the exercise, you were asked to jointly learn the embedding representation and the CNN convolution within the same network, which was the CNN counterpart of the left part of this Figure: \n",
    "\n",
    "<img src=\"embedding_or_RNN.png\" width=\"700px\" />\n",
    "\n",
    "Now, let's try to replace the RNN with a CNN for an architecture as on the right side.\n",
    "\n",
    "‚ùì **Question** ‚ùì Learn a word2vec model or load a trained one directly from GENSIM (transfer learning). Then, prepare your data as in the previous exercise. This question is quite long, it prepares you for real world challenges - but you have already done all the building bricks in the previous exercises"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "import numpy as np\n",
    "\n",
    "word2vec = Word2Vec(sentences=X_train, vector_size=100, min_count=3, window=10)\n",
    "\n",
    "# Function to convert a sentence (list of words) into a matrix representing the words in the embedding space\n",
    "def embed_sentence(word2vec, sentence):\n",
    "    embedded_sentence = []\n",
    "    for word in sentence:\n",
    "        if word in word2vec.wv:\n",
    "            embedded_sentence.append(word2vec.wv[word])\n",
    "        \n",
    "    return np.array(embedded_sentence)\n",
    "\n",
    "# Function that converts a list of sentences into a list of matrices\n",
    "def embedding(word2vec, sentences):\n",
    "    embed = []\n",
    "    \n",
    "    for sentence in sentences:\n",
    "        embedded_sentence = embed_sentence(word2vec, sentence)\n",
    "        embed.append(embedded_sentence)\n",
    "        \n",
    "    return embed\n",
    "\n",
    "# Embed the training and test sentences\n",
    "X_train_embed = embedding(word2vec, X_train)\n",
    "X_test_embed = embedding(word2vec, X_test)\n",
    "\n",
    "# Pad the training and test embedded sentences\n",
    "X_train_pad_2 = pad_sequences(X_train_embed, dtype='float32', padding='post', maxlen=150)\n",
    "X_test_pad_2 = pad_sequences(X_test_embed, dtype='float32', padding='post', maxlen=150)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì Now construct a model that has a `Conv1D` layer, a flatten layer, a dense layer, and an output layer. Compile it, and fit it on the train data. You can then evaluate it on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": [
     "challengify"
    ]
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "55/55 [==============================] - 0s 4ms/step - loss: 0.9215 - accuracy: 0.5263 - val_loss: 0.8156 - val_accuracy: 0.5413\n",
      "Epoch 2/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.5698 - accuracy: 0.7086 - val_loss: 0.8451 - val_accuracy: 0.5667\n",
      "Epoch 3/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.4522 - accuracy: 0.7903 - val_loss: 0.9031 - val_accuracy: 0.5733\n",
      "Epoch 4/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.3786 - accuracy: 0.8349 - val_loss: 0.9982 - val_accuracy: 0.5640\n",
      "Epoch 5/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.3140 - accuracy: 0.8703 - val_loss: 1.1454 - val_accuracy: 0.5787\n",
      "Epoch 6/20\n",
      "55/55 [==============================] - 0s 2ms/step - loss: 0.2444 - accuracy: 0.9029 - val_loss: 1.2191 - val_accuracy: 0.5867\n",
      "The accuracy evaluated on the test set is of 53.320%\n"
     ]
    }
   ],
   "source": [
    "model_cnn_2 = models.Sequential()\n",
    "\n",
    "model_cnn_2.add(layers.Conv1D(16, 3))\n",
    "model_cnn_2.add(layers.Flatten())\n",
    "model_cnn_2.add(layers.Dense(5,))\n",
    "model_cnn_2.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model_cnn_2.compile(loss='binary_crossentropy',\n",
    "             optimizer='rmsprop',\n",
    "             metrics='accuracy')\n",
    "\n",
    "model_cnn_2.fit(X_train_pad_2, y_train, \n",
    "          epochs=20, \n",
    "          batch_size=32,\n",
    "          validation_split=0.3,\n",
    "          callbacks=EarlyStopping(patience=5, restore_best_weights=True)\n",
    "         )\n",
    "\n",
    "res_2 = model_cnn_2.evaluate(X_test_pad_2, y_test, verbose=0)\n",
    "\n",
    "print(f'The accuracy evaluated on the test set is of {res_2[1]*100:.3f}%')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "‚ùì **Question** ‚ùì You might be frustrated by the accuracy you got, this happens to us all at some point. Once you have tested your first iteration, you need to improve your models: by making them more complex, changing the parameters, stacking additional layers, and so on.\n",
    "\n",
    "Only practice and experimentation will get you there. So you can go back to your previous models, change them and try to get better results ;)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.6 64-bit ('shims')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  },
  "vscode": {
   "interpreter": {
    "hash": "2352172e6ea988fd88dfcc10b834c5bcaa2937548cc931608b5edc504ebc2cb6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
